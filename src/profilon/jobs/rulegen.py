# src/profilon/jobs/rulegen.py

from __future__ import annotations

import os
import io
import re
import json
import time
import yaml
import hashlib
import datetime
from dataclasses import dataclass, field, asdict
from typing import Any, Dict, Iterable, List, Literal, Optional, Tuple

from databricks.sdk import WorkspaceClient
from databricks.labs.dqx.profiler.profiler import DQProfiler
from databricks.labs.dqx.profiler.dlt_generator import DQDltGenerator
from databricks.labs.dqx.engine import DQEngine
from databricks.labs.dqx.config import (
    FileChecksStorageConfig,
    WorkspaceFileChecksStorageConfig,
    TableChecksStorageConfig,  # kept for parity (not used for custom schema write)
    VolumeFileChecksStorageConfig,
)
from pyspark.sql import SparkSession, types as T

# --------------------------
# Supported profile keys (pass-through ok, but we highlight the "known" set)
# --------------------------
DOC_SUPPORTED_KEYS = {
    "sample_fraction", "sample_seed", "limit",
    "remove_outliers", "outlier_columns", "num_sigmas",
    "max_null_ratio", "trim_strings", "max_empty_ratio",
    "distinct_ratio", "max_in_count", "round",
}

# --------------------------
# Custom generated-checks table schema + doc (matches your notebook)
# --------------------------
DQX_GENERATED_CHECKS_CONFIG_METADATA: Dict[str, Any] = {
    "table": "dq_dev.dqx.generated_checks_config",  # overridden by actual FQN at write time
    "table_comment": (
        "## **DQX *Generated* Checks Configuration**\n"
        "- Stores flattened rules generated by the profiler.\n"
        "- Each row is a rule; `check_id` is a stable hash of the canonical payload.\n"
        "- `generator_meta` captures the profiler options and generator settings used to create these rows.\n"
    ),
    "columns": {
        "check_id": "SHA-256 hash of canonical payload (stable rule identity).",
        "check_id_payload": "Canonical JSON used to compute `check_id`.",
        "table_name": "Fully qualified target table (`catalog.schema.table`).",

        "name": "Human-readable rule name.",
        "criticality": "Rule severity: `warn|warning|error`.",
        "check": "Structured check object: {function, for_each_column, arguments}.",
        "filter": "Optional row-level filter expression.",
        "run_config_name": "Execution group/tag for this rule.",
        "user_metadata": "User-provided metadata map<string,string>.",

        "yaml_path": "YAML file path that held this rule (or `<generated://...>`).",
        "active": "If false, the rule is ignored by runners.",

        "generator_meta": (
            "Array of two items: "
            "`[{section:'profile_options', payload:map}, {section:'generator_settings', payload:map}]`."
        ),

        "created_by": "Audit: creator.",
        "created_at": "Audit: creation timestamp (UTC ISO).",
        "updated_by": "Audit: last updater.",
        "updated_at": "Audit: last update timestamp.",
    },
}

DQX_GENERATED_CHECKS_CONFIG_SCHEMA = T.StructType([
    T.StructField("check_id",            T.StringType(),  False),
    T.StructField("check_id_payload",    T.StringType(),  False),
    T.StructField("table_name",          T.StringType(),  False),

    T.StructField("name",                T.StringType(),  False),
    T.StructField("criticality",         T.StringType(),  False),
    T.StructField("check", T.StructType([
        T.StructField("function",        T.StringType(),  False),
        T.StructField("for_each_column", T.ArrayType(T.StringType()), True),
        T.StructField("arguments",       T.MapType(T.StringType(), T.StringType()), True),
    ]), False),
    T.StructField("filter",              T.StringType(),  True),
    T.StructField("run_config_name",     T.StringType(),  False),
    T.StructField("user_metadata",       T.MapType(T.StringType(), T.StringType()), True),

    T.StructField("yaml_path",           T.StringType(),  False),
    T.StructField("active",              T.BooleanType(), False),

    T.StructField("generator_meta", T.ArrayType(T.StructType([
        T.StructField("section", T.StringType(), False),  # "profile_options" | "generator_settings"
        T.StructField("payload", T.MapType(T.StringType(), T.StringType()), True),
    ])), True),

    T.StructField("created_by",          T.StringType(),  False),
    T.StructField("created_at",          T.StringType(),  False),  # ISO as string; cast later if needed
    T.StructField("updated_by",          T.StringType(),  True),
    T.StructField("updated_at",          T.StringType(),  True),
])

# --------------------------
# Small helpers
# --------------------------
def _safe_json(obj: Any) -> str:
    return json.dumps(obj, sort_keys=True, separators=(",", ":"))

def _now_iso() -> str:
    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

def _stringify_map_values(d: Optional[Dict[str, Any]]) -> Dict[str, str]:
    out: Dict[str, str] = {}
    for k, v in (d or {}).items():
        if isinstance(v, (list, dict)):
            out[k] = _safe_json(v)
        elif isinstance(v, bool):
            out[k] = "true" if v else "false"
        elif v is None:
            out[k] = "null"
        else:
            out[k] = str(v)
    return out

def _compute_check_id_payload(table_name: str, check_dict: Dict[str, Any], filter_str: Optional[str]) -> str:
    def _canon_filter(s: Optional[str]) -> str:
        return "" if not s else " ".join(str(s).split())

    def _canon_check(chk: Dict[str, Any]) -> Dict[str, Any]:
        out = {"function": chk.get("function"), "for_each_column": None, "arguments": {}}
        fec = chk.get("for_each_column")
        if isinstance(fec, list):
            out["for_each_column"] = sorted([str(x) for x in fec]) or None
        args = chk.get("arguments") or {}
        canon_args: Dict[str, str] = {}
        for k, v in args.items():
            sv = "" if v is None else str(v).strip()
            if (sv.startswith("{") and sv.endswith("}")) or (sv.startswith("[") and sv.endswith("]")):
                try:
                    sv = _safe_json(json.loads(sv))
                except Exception:
                    pass
            canon_args[str(k)] = sv
        out["arguments"] = {k: canon_args[k] for k in sorted(canon_args)}
        return out

    payload_obj = {
        "table_name": (table_name or "").lower(),
        "filter": _canon_filter(filter_str),
        "check": _canon_check(check_dict or {}),
    }
    return _safe_json(payload_obj)

def _compute_check_id(payload: str) -> str:
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()

def _is_yaml_path(p: str) -> bool:
    return bool(re.search(r"\.(ya?ml)$", p, re.IGNORECASE))

# --------------------------
# Storage helpers (Workspace Files / DBFS / Volumes / local)
# --------------------------
def _to_dbfs_target(path: str) -> str:
    if path.startswith("dbfs:/"):
        return path
    if path.startswith("/dbfs/") or path.startswith("/Volumes/"):
        return "dbfs:" + path
    return path

def _read_text_any(path: str) -> str:
    # DBFS / Volumes
    if path.startswith("dbfs:/") or path.startswith("/dbfs/") or path.startswith("/Volumes/"):
        try:
            from databricks.sdk.runtime import dbutils
        except Exception as e:
            raise RuntimeError("dbutils is required to read from DBFS/Volumes") from e
        target = path if path.startswith("dbfs:") else (f"dbfs:{path}" if path.startswith("/") else f"dbfs:/{path}")
        return dbutils.fs.head(target, 10 * 1024 * 1024)

    # Workspace Files (absolute)
    if path.startswith("/"):
        wc = WorkspaceClient()
        try:
            data = wc.files.download(file_path=path).read()
        except TypeError:
            data = wc.files.download(path=path).read()
        return data.decode("utf-8")

    # Local
    full = os.path.abspath(path)
    with open(full, "r", encoding="utf-8") as fh:
        return fh.read()

def _write_text_any(path: str, payload: str) -> None:
    # DBFS / Volumes
    if path.startswith("dbfs:/") or path.startswith("/dbfs/") or path.startswith("/Volumes/"):
        try:
            from databricks.sdk.runtime import dbutils
        except Exception:
            raise RuntimeError("dbutils is required to write to DBFS/Volumes.")
        target = path if path.startswith("dbfs:/") else (f"dbfs:{path}" if not path.startswith("dbfs:") else path)
        parent = target.rsplit("/", 1)[0]
        if parent:
            dbutils.fs.mkdirs(parent)
        dbutils.fs.put(target, payload, True)
        return

    # Workspace Files
    if path.startswith("/"):
        wc = WorkspaceClient()
        try:
            wc.files.upload(file_path=path, contents=payload.encode("utf-8"), overwrite=True)
        except TypeError:
            wc.files.upload(path=path, contents=payload.encode("utf-8"), overwrite=True)
        return

    # Local
    full = os.path.abspath(path)
    os.makedirs(os.path.dirname(full), exist_ok=True)
    with open(full, "w", encoding="utf-8") as fh:
        fh.write(payload)

# --------------------------
# Comments (table/column) with fallbacks
# --------------------------
def _esc_sql_comment(s: str) -> str:
    return (s or "").replace("'", "''")

def _apply_column_comment_with_fallback(
    spark: SparkSession,
    cat: str,
    sch: str,
    tbl: str,
    col_name: str,
    comment_text: str,
    col_types_lower: Dict[str, str],
) -> bool:
    fqn_q = f"`{cat}`.`{sch}`.`{tbl}`"
    col_q = f"`{col_name}`"
    cmt = _esc_sql_comment(comment_text)

    try:
        spark.sql(f"COMMENT ON COLUMN {fqn_q}.{col_q} IS '{cmt}'")
        return True
    except Exception:
        pass

    try:
        spark.sql(f"ALTER TABLE {fqn_q} ALTER COLUMN {col_q} COMMENT '{cmt}'")
        return True
    except Exception:
        pass

    dtype = col_types_lower.get(col_name.lower())
    if not dtype:
        return False
    try:
        spark.sql(f"ALTER TABLE {fqn_q} CHANGE COLUMN {col_q} {col_q} {dtype} COMMENT '{cmt}'")
        return True
    except Exception:
        return False

def _apply_table_documentation_on_create(
    spark: SparkSession, table_fqn: str, doc: Dict[str, Any], just_created: bool
):
    try:
        cat, sch, tbl = table_fqn.split(".")
    except ValueError:
        return

    table_comment = (doc or {}).get("table_comment") or ""
    if table_comment and just_created:
        spark.sql(f"COMMENT ON TABLE `{cat}`.`{sch}`.`{tbl}` IS '{_esc_sql_comment(table_comment)}'")

    cols_doc: Dict[str, str] = (doc or {}).get("columns") or {}
    if not cols_doc:
        return

    desc_rows = spark.sql(f"DESCRIBE TABLE `{cat}`.`{sch}`.`{tbl}`").collect()
    col_types = {}
    for r in desc_rows:
        if getattr(r, "col_name", None) and not str(r.col_name).startswith("#"):
            if getattr(r, "data_type", None):
                col_types[r.col_name.lower()] = r.data_type

    for col_name, cmt in cols_doc.items():
        _apply_column_comment_with_fallback(
            spark, cat, sch, tbl, col_name, cmt, col_types_lower=col_types
        )

# --------------------------
# Config model
# --------------------------
@dataclass
class RulegenConfig:
    # scope & discovery
    mode: Literal["pipeline", "catalog", "schema", "table"]
    name_param: str

    # sinks
    output_format: Literal["yaml", "table", "both"]
    output_location: Optional[str] = None        # legacy single sink
    output_yaml: Optional[str] = None            # preferred for yaml/both
    output_table: Optional[str] = None           # preferred for table/both

    # generation
    profile_options: Dict[str, Any] = field(default_factory=dict)
    exclude_pattern: Optional[str] = None        # glob on table name (e.g., ".tmp_*")
    exclude_prefix_regex: Optional[str] = None   # regex on prefix before first underscore
    created_by: str = "unknown"
    columns: Optional[List[str]] = None          # only valid when mode == "table" and single table
    run_config_name: str = "default"
    criticality: Literal["warn", "error"] = "warn"
    yaml_key_order: Literal["engine", "custom"] = "custom"
    include_table_name: bool = True
    yaml_metadata: bool = False                  # add a commented header to YAML

    # runtime metadata (optional)
    run_uuid: Optional[str] = None
    created_at: Optional[str] = None

    # optional generated table documentation override
    table_doc: Optional[Dict[str, Any]] = None   # defaults to DQX_GENERATED_CHECKS_CONFIG_METADATA

    def fingerprint(self) -> str:
        raw = yaml.safe_dump(asdict(self), sort_keys=True, default_flow_style=False)
        return hashlib.sha256(raw.encode("utf-8")).hexdigest()

# --------------------------
# Core generator
# --------------------------
class RuleGenerator:
    """
    Generate DQX checks and write to YAML, a table with the custom schema,
    or BOTH (YAML first, then load exactly those YAMLs into the table).
    """

    def __init__(self, cfg: RulegenConfig):
        self.cfg = cfg
        self.spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()

        # Sinks validation (+ back-compat for output_location)
        fmt = self.cfg.output_format.lower().strip()
        if fmt not in {"yaml", "table", "both"}:
            raise ValueError("output_format must be 'yaml', 'table', or 'both'.")

        # Back-compat helpers
        if fmt == "yaml" and not (self.cfg.output_yaml or self.cfg.output_location):
            raise ValueError("When output_format='yaml', provide output_yaml (or output_location).")
        if fmt == "table" and not (self.cfg.output_table or self.cfg.output_location):
            raise ValueError("When output_format='table', provide output_table (or output_location).")
        if fmt == "both":
            if not (self.cfg.output_yaml or None) or not (self.cfg.output_table or None):
                # try to infer from legacy single output_location? (too ambiguous); require explicit
                if not (self.cfg.output_yaml and self.cfg.output_table):
                    raise ValueError("When output_format='both', provide output_yaml and output_table.")

        self._dq_engine = DQEngine(WorkspaceClient())
        self._profiler = DQProfiler(WorkspaceClient())
        self._generator = DQDltGenerator(WorkspaceClient())
        self._table_doc = self.cfg.table_doc or DQX_GENERATED_CHECKS_CONFIG_METADATA

    # ---------- profile kw ----------
    def _profile_call_kwargs(self) -> Dict[str, Any]:
        kwargs: Dict[str, Any] = {}
        if self.cfg.columns is not None:
            if self.cfg.mode != "table":
                raise ValueError("The 'columns' parameter is only valid when mode='table'.")
            kwargs["cols"] = self.cfg.columns
        if self.cfg.profile_options:
            unknown = sorted(set(self.cfg.profile_options) - DOC_SUPPORTED_KEYS)
            if unknown:
                print(f"[INFO] Profiling options not in current docs (passing through anyway): {unknown}")
            kwargs["options"] = self.cfg.profile_options
        return kwargs

    # ---------- discovery ----------
    def _exclude_tables_by_pattern_glob(self, fq_tables: List[str]) -> List[str]:
        if not self.cfg.exclude_pattern:
            return fq_tables
        pat = self.cfg.exclude_pattern
        if not pat.startswith("."):
            raise ValueError("Exclude pattern must start with a dot, e.g. '.tmp_*'")
        glob = re.escape(pat[1:]).replace(r"\*", ".*")
        rx = re.compile("^" + glob + "$")
        kept = [fq for fq in fq_tables if not rx.match(fq.split(".")[-1])]
        print(f"[INFO] Excluded {len(fq_tables) - len(kept)} tables by pattern '{pat}'")
        return kept

    @staticmethod
    def _prefix_of(table_fqn: str) -> str:
        base = table_fqn.split(".")[-1]
        return base.split("_", 1)[0].lower() if base else ""

    def _filter_by_prefix_regex(self, tables: List[str]) -> List[str]:
        rx = self.cfg.exclude_prefix_regex
        if not rx:
            return tables
        pat = re.compile(rx, re.IGNORECASE)
        return [t for t in tables if not pat.search(self._prefix_of(t))]

    def _discover_tables(self) -> List[str]:
        print("\n===== RULEGEN CONFIG =====")
        print(json.dumps({
            **{k: v for k, v in asdict(self.cfg).items() if k != "profile_options"},
            "profile_options": self.cfg.profile_options,
        }, indent=2))
        print("==========================\n")

        discovered: List[str] = []
        mode = self.cfg.mode

        if mode == "pipeline":
            print("Discovering pipeline output tables...")
            ws = WorkspaceClient()
            targets = [p.strip() for p in self.cfg.name_param.split(",") if p.strip()]
            pls = list(ws.pipelines.list_pipelines())
            for pipeline_name in targets:
                pl = next((p for p in pls if p.name == pipeline_name), None)
                if not pl:
                    raise RuntimeError(f"Pipeline '{pipeline_name}' not found via SDK.")
                latest_update = pl.latest_updates[0].update_id
                events = ws.pipelines.list_pipeline_events(pipeline_id=pl.pipeline_id, max_results=250)
                pipeline_tables = [
                    getattr(ev.origin, "flow_name", None)
                    for ev in events
                    if getattr(ev.origin, "update_id", None) == latest_update
                    and getattr(ev.origin, "flow_name", None)
                ]
                discovered += [x for x in pipeline_tables if x]

        elif mode == "catalog":
            catalog = self.cfg.name_param.strip()
            schemas = [row.namespace for row in self.spark.sql(f"SHOW SCHEMAS IN {catalog}").collect()]
            for s in schemas:
                tbls = self.spark.sql(f"SHOW TABLES IN {catalog}.{s}").collect()
                discovered += [f"{catalog}.{s}.{row.tableName}" for row in tbls]

        elif mode == "schema":
            if self.cfg.name_param.count(".") != 1:
                raise ValueError("For 'schema' mode, name_param must be catalog.schema")
            catalog, schema = self.cfg.name_param.strip().split(".")
            tbls = self.spark.sql(f"SHOW TABLES IN {catalog}.{schema}").collect()
            discovered = [f"{catalog}.{schema}.{row.tableName}" for row in tbls]

        else:  # table
            tables = [t.strip() for t in self.cfg.name_param.split(",") if t.strip()]
            bad = [t for t in tables if t.count(".") != 2]
            if bad:
                raise ValueError(f"Tables must be fully qualified (catalog.schema.table). Invalid: {bad}")
            discovered = tables

        discovered = sorted(set(self._exclude_tables_by_pattern_glob(discovered)))
        discovered = self._filter_by_prefix_regex(discovered)

        print("[INFO] Final table list:")
        for t in discovered:
            print(" -", t)
        return discovered

    # ---------- DQX storage helpers ----------
    @staticmethod
    def _infer_file_storage_config(path: str):
        if path.startswith("/Volumes/"):
            return VolumeFileChecksStorageConfig(location=path)
        if path.startswith("/"):
            return WorkspaceFileChecksStorageConfig(location=path)
        if path.startswith("dbfs:/") or path.startswith("/dbfs/") or path.startswith("dbfs:"):
            return FileChecksStorageConfig(location=path if path.startswith("dbfs:") else f"dbfs:{path}")
        return FileChecksStorageConfig(location=os.path.abspath(path))

    # ---------- shaping ----------
    def _dq_constraint_to_check(
        self, rule_name: str, constraint_sql: str, table_name: str
    ) -> Dict[str, Any]:
        d = {
            "name": rule_name,
            "criticality": self.cfg.criticality,
            "run_config_name": self.cfg.run_config_name,
            "check": {
                "function": "sql_expression",
                "arguments": {"expression": constraint_sql},
            },
        }
        if self.cfg.include_table_name:
            d = {"table_name": table_name, **d}
        return d

    # ---------- YAML emission ----------
    def _yaml_header_block(self, table_fqn: str) -> str:
        lines = [
            "#" * 76,
            f"# GENERATED DQX CHECKS",
            f"# Table: {table_fqn}",
            f"# Generated at (UTC): {_now_iso()}",
            "#",
            "# Profile options:",
            "# " + _safe_json(self.cfg.profile_options),
            "# Generator settings:",
            "# " + _safe_json({
                "mode": self.cfg.mode,
                "name_param": self.cfg.name_param,
                "output_format": self.cfg.output_format,
                "output_yaml": self.cfg.output_yaml or "",
                "output_table": self.cfg.output_table or "",
                "criticality": self.cfg.criticality,
                "run_config_name": self.cfg.run_config_name,
                "include_table_name": self.cfg.include_table_name,
                "key_order": self.cfg.yaml_key_order,
                "exclude_prefix_regex": self.cfg.exclude_prefix_regex or "",
            }),
            "#" * 76,
            "",
        ]
        return "\n".join(lines) + "\n"

    @staticmethod
    def _dump_rules_as_yaml_stream(rules: List[Dict[str, Any]]) -> str:
        pieces: List[str] = []
        for r in rules:
            block = yaml.safe_dump(r, sort_keys=False, default_flow_style=False).rstrip()
            if not block:
                continue
            lines = block.splitlines()
            first = f"- {lines[0]}"
            rest = "\n".join(("  " + ln) for ln in lines[1:])
            pieces.append(first + ("\n" + rest if rest else ""))
        return "\n\n".join(pieces) + "\n"

    # ---------- table write ----------
    def _ensure_schema_exists(self, fqn: str):
        cat, sch, _ = fqn.split(".")
        self.spark.sql(f"CREATE SCHEMA IF NOT EXISTS `{cat}`.`{sch}``")

    def _write_rows_to_table(self, fqn: str, rows: List[Dict[str, Any]], mode: str = "append"):
        self._ensure_schema_exists(fqn)
        existed = self.spark.catalog.tableExists(fqn)
        if not existed:
            empty_df = self.spark.createDataFrame([], DQX_GENERATED_CHECKS_CONFIG_SCHEMA)
            empty_df.write.format("delta").mode("overwrite").saveAsTable(fqn)
        _apply_table_documentation_on_create(self.spark, fqn, {**self._table_doc, "table": fqn}, just_created=(not existed))

        df = self.spark.createDataFrame(rows, schema=DQX_GENERATED_CHECKS_CONFIG_SCHEMA)
        df.write.format("delta").mode(mode).saveAsTable(fqn)
        print(f"[WRITE] {len(rows)} rows -> {fqn} ({mode})")

    # ---------- main ----------
    def run(self) -> "RunSummary":
        tables = self._discover_tables()
        call_kwargs = self._profile_call_kwargs()

        fmt = self.cfg.output_format
        out_yaml = self.cfg.output_yaml or (self.cfg.output_location if fmt == "yaml" else None)
        out_table = self.cfg.output_table or (self.cfg.output_location if fmt == "table" else None)

        written_yaml_paths: List[str] = []
        all_rows_for_table_sink: List[Dict[str, Any]] = []
        total_checks = 0
        warnings: List[str] = []
        errors: List[str] = []

        for fq_table in tables:
            try:
                # sanity read to ensure permissions
                self.spark.table(fq_table).limit(1).collect()
            except Exception as e:
                w = f"[WARN] Table not readable: {fq_table} ({e})"
                print(w); warnings.append(w); continue

            # profile + generate
            try:
                df = self.spark.table(fq_table)
                print(f"[RUN] Profiling: {fq_table}")
                _, profiles = self._profiler.profile(df, **call_kwargs)
                rules_dict = self._generator.generate_dlt_rules(profiles, language="Python_Dict")
            except Exception as e:
                w = f"[WARN] Profiling/rule-gen failed for {fq_table}: {e}"
                print(w); warnings.append(w); continue

            # shape checks
            checks: List[Dict[str, Any]] = []
            for rule_name, constraint in (rules_dict or {}).items():
                checks.append(self._dq_constraint_to_check(rule_name, constraint, fq_table))
            if not checks:
                print(f"[INFO] No checks generated for {fq_table}.")
                continue

            # --- YAML sink ---
            yaml_path_for_rows = f"<generated://{fq_table}>"
            if fmt in {"yaml", "both"}:
                if out_yaml is None:
                    raise ValueError("output_yaml is required when writing YAML.")
                cat, sch, tab = fq_table.split(".")
                path = out_yaml if out_yaml.endswith((".yaml", ".yml")) else f"{out_yaml.rstrip('/')}/{tab}.yaml"
                if self.cfg.yaml_key_order == "engine":
                    cfg = self._infer_file_storage_config(path)
                    print(f"[RUN] Saving {len(checks)} checks via DQX to: {path}")
                    self._dq_engine.save_checks(checks, config=cfg)
                else:
                    header = self._yaml_header_block(fq_table) if self.cfg.yaml_metadata else ""
                    body = self._dump_rules_as_yaml_stream(checks)
                    print(f"[RUN] Saving {len(checks)} checks (ordered YAML) to: {path}")
                    _write_text_any(path, header + body)

                yaml_path_for_rows = path
                written_yaml_paths.append(path)
                total_checks += len(checks)

            # --- prepare rows for table-only path ---
            if fmt == "table":
                gen_meta = [
                    {"section": "profile_options", "payload": _stringify_map_values(self.cfg.profile_options)},
                    {"section": "generator_settings", "payload": _stringify_map_values({
                        "mode": self.cfg.mode, "name_param": self.cfg.name_param, "output_format": self.cfg.output_format,
                        "output_yaml": out_yaml or "", "output_table": out_table or "",
                        "criticality": self.cfg.criticality, "run_config_name": self.cfg.run_config_name,
                        "include_table_name": self.cfg.include_table_name, "key_order": self.cfg.yaml_key_order,
                        "exclude_prefix_regex": self.cfg.exclude_prefix_regex or "",
                    })},
                ]
                for rule in checks:
                    raw_check = rule["check"]
                    payload = _compute_check_id_payload(fq_table, raw_check, rule.get("filter"))
                    all_rows_for_table_sink.append({
                        "check_id": _compute_check_id(payload),
                        "check_id_payload": payload,
                        "table_name": fq_table,
                        "name": rule["name"],
                        "criticality": rule["criticality"],
                        "check": {
                            "function": raw_check.get("function"),
                            "for_each_column": raw_check.get("for_each_column"),
                            "arguments": _stringify_map_values(raw_check.get("arguments") or {}),
                        },
                        "filter": rule.get("filter"),
                        "run_config_name": rule["run_config_name"],
                        "user_metadata": _stringify_map_values(rule.get("user_metadata") or None) or None,
                        "yaml_path": yaml_path_for_rows,
                        "active": True,
                        "generator_meta": gen_meta,
                        "created_by": self.cfg.created_by,
                        "created_at": _now_iso(),
                        "updated_by": None,
                        "updated_at": None,
                    })

        # BOTH: reload YAMLs and write canonical rows
        if fmt == "both":
            if not out_table:
                raise ValueError("output_table is required when output_format='both'.")
            rows_from_yaml: List[Dict[str, Any]] = []
            for yp in written_yaml_paths:
                try:
                    txt = _read_text_any(yp)
                    docs = list(yaml.safe_load_all(io.StringIO(txt)))
                    rules: List[dict] = []
                    for d in docs:
                        if not d:
                            continue
                        if isinstance(d, dict):
                            rules.append(d)
                        elif isinstance(d, list):
                            rules.extend([x for x in d if isinstance(x, dict)])
                    for r in rules:
                        fq = r.get("table_name")
                        raw_check = r.get("check") or {}
                        payload = _compute_check_id_payload(fq, raw_check, r.get("filter"))
                        rows_from_yaml.append({
                            "check_id": _compute_check_id(payload),
                            "check_id_payload": payload,
                            "table_name": fq,
                            "name": r.get("name"),
                            "criticality": r.get("criticality"),
                            "check": {
                                "function": raw_check.get("function"),
                                "for_each_column": raw_check.get("for_each_column"),
                                "arguments": _stringify_map_values(raw_check.get("arguments") or {}),
                            },
                            "filter": r.get("filter"),
                            "run_config_name": r.get("run_config_name", self.cfg.run_config_name),
                            "user_metadata": _stringify_map_values(r.get("user_metadata") or None) or None,
                            "yaml_path": yp,
                            "active": True,
                            "generator_meta": [
                                {"section": "profile_options", "payload": _stringify_map_values(self.cfg.profile_options)},
                                {"section": "generator_settings", "payload": _stringify_map_values({
                                    "mode": self.cfg.mode, "name_param": self.cfg.name_param, "output_format": self.cfg.output_format,
                                    "output_yaml": out_yaml or "", "output_table": out_table or "",
                                    "criticality": self.cfg.criticality,
                                    "run_config_name": r.get("run_config_name", self.cfg.run_config_name),
                                    "include_table_name": self.cfg.include_table_name,
                                    "key_order": self.cfg.yaml_key_order,
                                    "exclude_prefix_regex": self.cfg.exclude_prefix_regex or "",
                                })},
                            ],
                            "created_by": self.cfg.created_by,
                            "created_at": _now_iso(),
                            "updated_by": None,
                            "updated_at": None,
                        })
                except Exception as e:
                    warnings.append(f"[WARN] Could not load YAML '{yp}' → {e}")

            if rows_from_yaml:
                self._write_rows_to_table(out_table, rows_from_yaml, mode="append")
            print(f"[DONE] Wrote YAMLs ({len(written_yaml_paths)}). Then loaded {len(rows_from_yaml)} rows into {out_table}.")

        elif fmt == "table":
            if not out_table:
                raise ValueError("output_table is required when output_format='table'.")
            if all_rows_for_table_sink:
                self._write_rows_to_table(out_table, all_rows_for_table_sink, mode="append")
            print(f"[DONE] Wrote {len(all_rows_for_table_sink)} rows into {out_table}.")
        else:
            print(f"[DONE] Wrote YAML files ({len(written_yaml_paths)}).")

        return RunSummary(
            tables=tables,
            total_checks=total_checks,
            output_format=fmt,
            output_yaml=out_yaml,
            output_table=out_table,
            written_files=written_yaml_paths,
            warnings=warnings,
            errors=errors,
        )

# --------------------------
# Summary DTO
# --------------------------
@dataclass
class RunSummary:
    tables: List[str] = field(default_factory=list)
    total_checks: int = 0
    output_format: Literal["yaml", "table", "both"] = "yaml"
    output_yaml: Optional[str] = None
    output_table: Optional[str] = None
    written_files: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)